---
title: "Part 4"
author: "Nick George"
date: "5/3/2018"
output: pdf_document
---
## Nick George and Spencer Louie 
```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.width=6, fig.height=3)
require(openintro)
require(dplyr)
require(broom)
require(ggplot2)
require(skimr)
require(readr)
require(glmnet)
require(splines)
require(car)
require(gam)
kc_data <- read.table("~/MATH158Proj/Part 1/kc_house_data.csv", header=TRUE,
   sep=",")
data("kc_house_data")
kc_data <- kc_data %>% mutate(bedbath = bedrooms + bathrooms)
kc_data <- kc_data %>% mutate(lprice = log(price))
kc_data <- kc_data[-(15871),]
kc_data<- kc_data %>% mutate(bedbathi = round(bedbath, digits=0))
dt = sort(sample(nrow(kc_data),nrow(kc_data)*.99))
kc_data.train <- kc_data[dt,]
kc_data.test <- kc_data[-dt,]
dt2= sort(sample(nrow(kc_data),nrow(kc_data)*.75))
kc_data.train2 <- kc_data[dt,]
kc_data.test2 <- kc_data[-dt,]
```
##Introduction
  This study is based on a data set of home sales in King County, Washington from 2014 to 2015 (May) and comes from the Center for Spatial Data Science. The relevant variables are price, the sale price of the home; sqft_living, the square footage of living space; sqft_lot, the square footage of lot space; the number of floors; whether the property is watefront; the number of bedrooms/bathrooms, and the condition of the house (based on King County's grading system). From this data we are hoping to infer more about the general population of home sales in the U.S. As such specific variables like condition, will be extrapalated as roughly how much the structure of the house matters rather than the specific value because it is based on King County's system. The goal of this research is to figure out which factors are important in deciding the sale price of a home.  

We ran ridge regression and lasso models to compare coefficients with our MLR model. The two lists below show the RR and lasso coefficients, respectively.  
```{r, echo=FALSE}
lambda.grid <- 10^seq(5,-5, length=100)
kc_4_Xdata <- kc_data.train[-c(1, 2, 3, 16, 17, 18, 19, 20, 21)]
kc_4_Xdata <- kc_4_Xdata %>% mutate(lsqft_living <- log(sqft_living))
kc_4_Xdata <- kc_4_Xdata %>% mutate(lsqft_lot <- log(sqft_lot))
kc_4_Xdata <- kc_4_Xdata %>% mutate(yr_built.adj <- yr_built - 1899)
kc_4_Xdata.final <- kc_4_Xdata[-c(1,2,3,4,10, 12, 14)]
kc_4_Xmatrix <- data.matrix(kc_4_Xdata.final)
kc_rr.cv<-cv.glmnet(kc_4_Xmatrix, log(kc_data.train$price), alpha=0, lambda=lambda.grid)
coef(kc_rr.cv, s = "lambda.min")
kc_lasso.cv<-cv.glmnet(kc_4_Xmatrix, log(kc_data.train$price), alpha=1, lambda=lambda.grid)
coef(kc_lasso.cv, s = "lambda.min")

```
  
  The ridge regression model suggests that we use all of the coefficients just as we would expect. The lasso model similarly suggests we use all of the variables which includes one more than we did in our own model. It suggests that we should include the amount of square footage in the basement, as well as the year the home was built. Note that the year built variable has been slightly adjusted so that 1900, the oldest home, is considered year 1 and then goes up normally from there. We then compare this with our full multiple linear regression model. Also because ridge regression and lasso both reccomend using all of the variables their coefficient estimates are nearly identical.  

```{r, echo=FALSE}
kc_mlr.lm <- lm(log(price) ~  floors + waterfront + view + condition + grade + bedbath + log(sqft_living) + log(sqft_lot), data= kc_data.train)
round(coef(kc_mlr.lm), digits=3)
```

  All of the coefficients for predictors in all three models are the same across the model in terms of direction, though not necessarily in magnitude (but not great changes in magnitude either). Except for floors, which went from having a negative coefficient in our multiple linear regression model to a postive one in the lasso and ridge regression models. We originally did not include the variables added in these models for logistical reasons. First the amount of squarefootage in the basement was problematic because it did not include a way to qualify when the basement was finished or not. Furthermore for the year the house was built, some of the homes have had renovations, but we have no way of qualifying the quality or scale of renovations. And due to those problems we did not originally include those predictors.  

```{r, echo=FALSE}
kc_4_Xdata.test <- kc_data.test[-c(1, 2, 3, 16, 17, 18, 19, 20, 21)]
kc_4_Xdata.test <- kc_4_Xdata.test %>% mutate(lsqft_living <- log(sqft_living))
kc_4_Xdata.test <- kc_4_Xdata.test %>% mutate(lsqft_lot <- log(sqft_lot))
kc_4_Xdata.test <- kc_4_Xdata.test %>% mutate(yr_built.adj <- yr_built - 1899)
kc_4_Xdata.final.test <- kc_4_Xdata.test[-c(1,2,3,4,10, 12, 14)]
kc_4_Xmatrix.test <- data.matrix(kc_4_Xdata.final.test)
plot(log(kc_data.test$price), predict(kc_mlr.lm, newdata=kc_data.test), type="p", xlab= "Observed Log(Price)", ylab= "Predicted Log(Price)")
title(main="Observed Price v. Predicted Price")
points(log(kc_data.test$price), predict(kc_rr.cv, newx=kc_4_Xmatrix.test, s= "lambda.min"), col="blue")
points(log(kc_data.test$price), predict(kc_lasso.cv, newx=kc_4_Xmatrix.test, s= "lambda.min"), col="red")
```


  This graph shows the true values of the log of price on the x axis and the fitted values for each of the regression types on the y axis. Ideally we would a one-to-one relationship or a slope of 1.. In black are multiple linear regression values, in blue are the ridge regression values and in red are the lasso values. The data plotted comes from test data and the model was built using training data. We split our data into those two groups becasue we have so many observations. That is also why the plot has far fewer points than the overall data set. This separation allows us to better analyze how accurate our predictions are. The lasso and ridge regression models are very similar as you can see in the table with their coefficients, so in most places they are just overtop of each other. The black multiple linear regression points are also quite close to their blue/red counterparts implying that the models predict fairly similarly. Overall we have decided to stick with our originally multiple linear regression model because we are warry of the two logical reasons for excluding the two added variables in ridge regression and lasso and including them may cause some kind of unintended bias. 


```{r, echo=FALSE}
lotlims <- range(kc_data$sqft_lot)
lot.grid <-seq(from=lotlims[1], to=lotlims[2])
lot.grid2 <-seq(from=lotlims[1], to=lotlims[2], by=100)
```

```{r, echo=FALSE}
kc.rs1 <- lm(price ~ bs(sqft_lot, degree=3, knots=c(100000, 200000, 300000)), data=kc_data)
kc.rs1.pred <- predict(kc.rs1, newdata=list(sqft_lot=lot.grid), se=TRUE)
kc.rs1.se <- cbind(kc.rs1.pred$fit + 2*kc.rs1.pred$se.fit,
                   kc.rs1.pred$fit - 2*kc.rs1.pred$se.fit)
kc.rs2 <- lm(price ~ bs(sqft_lot, degree=3, knots=c(10000, 100000, 200000, 300000)), data=kc_data)
kc.rs2.pred <- predict(kc.rs2, newdata=list(sqft_lot=lot.grid), se=TRUE)
kc.rs2.se <- cbind(kc.rs2.pred$fit + 2*kc.rs2.pred$se.fit,
                   kc.rs2.pred$fit - 2*kc.rs2.pred$se.fit)
kc.rs3 <- lm(price ~ bs(sqft_lot, degree=5, knots=c(100000, 200000, 300000)), data=kc_data)
kc.rs3.pred <- predict(kc.rs3, newdata=list(sqft_lot=lot.grid), se=TRUE)
kc.rs3.se <- cbind(kc.rs3.pred$fit + 2*kc.rs3.pred$se.fit,
                   kc.rs3.pred$fit - 2*kc.rs3.pred$se.fit)
kc.rs4 <- lm(price ~ bs(sqft_lot, degree=3, knots=c(100000, 200000, 300000, 500000)), data=kc_data)
kc.rs4.pred <- predict(kc.rs4, newdata=list(sqft_lot=lot.grid), se=TRUE)
kc.rs4.se <- cbind(kc.rs4.pred$fit + 2*kc.rs4.pred$se.fit,
                   kc.rs4.pred$fit - 2*kc.rs4.pred$se.fit)

```

```{r, echo=FALSE}
plot(kc_data$sqft_lot, kc_data$price, cex=.5, pch=19, col="darkgrey", xlab="Squarefoot Lot", ylab="Price", xlim=lotlims)
title("Regression Spline", outer= F)
lines(lot.grid, kc.rs1.pred$fit, lwd=2, col="blue")
lines(lot.grid, kc.rs2.pred$fit, lwd=2, col="red")
lines(lot.grid, kc.rs3.pred$fit, lwd=2, col="green")
lines(lot.grid, kc.rs4.pred$fit, lwd=2, col="orange")


```
  
  The graph above illustrates four different possible regression splines for square footage of lot against price. All four models contain knots at 100,000, 200,000 and 300,000 in an effort to better explain the early variability. The blue line contains those three knots at degree 3. The red line contains those three knots as well as another early knot at 10,000, at degree 3 in an effort to sort out the variability we see extremely early on. The orange line adds a knot at the end of the blue model, at 750,000 to look at the differences at the higher levels. Lastly, the red line contains the blue model's knots at degree 5. We see that all four models seem to be fairly similar, but each has a slight difference based on the added portion to it. For example the orange is line is far more straight in the middle to end with it's knot at 750,000 and the green line has more curvature due to it's increased degree.  

```{r, echo=FALSE}
## This code takes a long time to run. 
kc.lor1 <- loess(price ~ sqft_lot, span=.2, data=kc_data)
kc.lor1.pred <- predict(kc.lor1,newdata=data.frame(sqft_lot=lot.grid2), se=TRUE)
kc.lor1.se <- cbind(kc.lor1.pred$fit + 2*kc.lor1.pred$se.fit,
                    kc.lor1.pred$fit - 2*kc.lor1.pred$se.fit)
kc.lor2 <- loess(price ~ sqft_lot, span=.5, data=kc_data)
kc.lor2.pred <- predict(kc.lor2,newdata=data.frame(sqft_lot=lot.grid2), se=TRUE)
kc.lor2.se <- cbind(kc.lor2.pred$fit + 2*kc.lor2.pred$se.fit,
                    kc.lor2.pred$fit - 2*kc.lor2.pred$se.fit)
kc.lor3 <- loess(price ~ sqft_lot, span=.7, data=kc_data)
kc.lor3.pred <- predict(kc.lor3,newdata=data.frame(sqft_lot=lot.grid2), se=TRUE)
kc.lor3.se <- cbind(kc.lor3.pred$fit + 2*kc.lor3.pred$se.fit,
                    kc.lor3.pred$fit - 2*kc.lor3.pred$se.fit)
kc.lor4 <- loess(price ~ sqft_lot, span=1, data=kc_data)
kc.lor4.pred <- predict(kc.lor4,newdata=data.frame(sqft_lot=lot.grid2), se=TRUE)
kc.lor4.se <- cbind(kc.lor4.pred$fit + 2*kc.lor4.pred$se.fit,
                    kc.lor4.pred$fit - 2*kc.lor4.pred$se.fit)
```

```{r, echo=FALSE}
plot(kc_data$sqft_lot, kc_data$price, cex = .5, pch=19, col = 'darkgrey', xlab = 'Squarefoot Lot', ylab='Price', xlim=lotlims)
title('Local Regression (Loess)', outer=F)
lines(lot.grid2, kc.lor1.pred$fit, lwd=2, col="blue")
lines(lot.grid2, kc.lor2.pred$fit, lwd=2, col="red")
lines(lot.grid2, kc.lor3.pred$fit, lwd=2, col="green")
lines(lot.grid2, kc.lor4.pred$fit, lwd=2, col="orange")

```

  The graph above shows another way to smooth our curve in a local regression method called Loess. The four lines predict on the same variables for the same values, but differ in the span for each curver. The blue curve has a span of .2, the red curve has a span of .5, the green curve has a span of .7, and the orange curve has a span of 1. The span effects the curve in that it dictates how many of the nearby points are used to predict the line, amongst those points the ones closest to it are given higher weights, and that is again dependent on the span. The smaller the span, the closer the points have to be to be considered in the local regression, and the lower the weight they are given. We see that he blue, orange, and red lines seem to fit the data fairly well, while the green line seems to be fairly off. This likely is because the points that were used for the local regression, particularly in the middle, caused a strange fit. The blue and red curve's use only the data right around each point to predict to it seems to fit the data well. The orange curve uses all of the data and runs the regression (though with weights) and also seems to fit alright.  

  Overall we would the blue Loess model to predict future values. We think it is important to have a low span so that you are only regressing using the points right around the point in question. This is especially true for this variable because the data does not have a clear functional form. That is also why we are not choosing to use regression splines because, again, there is not a clear functional form for the data, so doing a local regression seems more beneficial.  

##Conclusion
  Ridge regression and lasso yielded the results we were expecting. With the number of observations far outweighing the number of predictors, it was expected that the RR and lasso to find all variables significant to the model. However, due to the way the variables are constructed it makes more sense to use the MLR that was determined from the last part of the project. Further, given the type and number of variables we have, RR or lasso are solving a problem that is not present in the data.  

  Loess and splines also yielded expected results. Choosing the appropriate number and location of knots was an important determination, but ultimately the spline curves looked quite similar. Much more important was choosing the span of the Loess, which made the smoothed curve vary widely. Overall, smooth regressions are much more appropriate for the kind of data we have compared with sparse models. Choosing an appropriate smooth regression function yielded curves that fit the data well. Loess seemed to be the optimal method because it did not attempt to fit an actual spline to it rather running individual local regressions along the way. We also picked the smallest span so that only points immediately surrounding the highlighted part would be considered in the local regression.


##Added Variable Plots
  One factor that we are particularly cognisant of in our research is the fit of our model. Without an approriate fit the inference of our predictors is fairly meaningless and inaccurate. It is also important because the shape of our model is not necesarilly obvious. So we have decided to look at a few things to help us ensure an appropriate model. First, we decided to look at added varaible plots. The premise behind these plots is to see how and if an additional predictor matters when you consider the other predictors already included in the model. We are more interested in the how part of the added variable plot than the if. The plot gives us the ability to again help evaluate fit. We can see if the fit is linear or something else. Fit is extremely important for our model and any model, because if the model does not accurately represent the data then the inferences from it will be innaccurate. Our model fit is particularly important because it is not obviously linear as is, and to address that we have already made several transformations to the data. The plots show the residuals of one variale given that the other variables are in teh model. Each point is $e_i(Y|X_2+X_3+X_4) = Y_i - \hat{Y}(X_2) - \hat{Y}(X_3) - \hat{Y}(X_4)$ on the Y axis. On the X axis it is, $e_i(X_1|X_2+X_3+X_4) = X_{i1} - \hat{X_{i1}}(X_2) - \hat{X_{i1}}(X_3) - \hat{X_{i1}}(X_4)$ In order to get these points we are doing a regression so we do require the normal assumptions or technical conditions for a regression, normality and constant variance for error terms, independence and linearity. With our dataset being so large we are able to satisfy the normality condition and we have checked for the previous assumption earlier.  

```{r, echo=FALSE}
kc4.lm.full1<- lm(price ~ sqft_living + sqft_lot + factor(floors) + factor(waterfront) + factor(view) + factor(condition) + factor(grade) + factor(bedbath), data=kc_data.train)
kc4.lm.full2<- lm(log(price) ~ log(sqft_living) + log(sqft_lot) + factor(floors) + factor(waterfront) + factor(view) + factor(condition) + factor(grade) + factor(as.integer(bedbath)), data=kc_data.train)
kc4.lm.red1<- lm(price ~ sqft_living + sqft_lot + floors + waterfront + view + condition + grade + bedbath, data=kc_data.train)
kc4.lm.red2<- lm(log(price) ~ log(sqft_living) + log(sqft_lot) + floors + waterfront + view + log(condition) + grade + bedbathi, data=kc_data.train)

par(mfrow=c(1,2))

avPlots(kc4.lm.red2, terms=~grade)
avPlots(kc4.lm.red2, terms=~bedbathi)
```

  Here we have a couple added variable plots for a couple variables of interest. Since none of them have a horizontal line we do see a relationship for all of them and therefore each one adds something to the plot. Bedbath seems to have a negative relationship while grade has a positive relationship. We also see the the lines themselves as well as the points seem to imply linear relationships, the lines are straight and the points are not mostly above or below the line at certain parts, rather they are even distributed at ever segement of the line.  
  
```{r, echo=FALSE}
avPlots(lm(log(price) ~ sqft_living + log(sqft_lot) + floors + waterfront + view + log(condition) + grade + bedbath, data=kc_data.train), terms=~sqft_living)
```
  
  Here we have a graph with all parts of the model included except for sqft_living, which we are looking at without a transformation. We do see that the line is straight, but the points are not as evenly distributed as we have seen the in past. Earlier and later on it seems more points are below and in the middle more are above. The curvature implies we should use a transformation, which we have done. These graphs are fairly hard to read since they include all of the data in the training set, so we have decided to include another method to more accurately and directly evaluate fit.  

##Lack of Fit

  We decided to look at the Lack of Fit test. This is an F-Test with the simple goal of evaluating fit. One of the convenient aspects of this test is that it does not require any additional assumptions then the one we have used to run our regressions as it is simply looking at two regressions. In fact, it does not even require linearity as that is what it is meant to test for. This test uses a nested F-test, so it is comprised of both a full model and a reduced model. The full model looks at a regression where instead of assuming a linear relationship it counts each variable as a factor variable, so it measures the mean Y at each level of a variable. In other words what is the average value of Y when X1=1. The reduced model is the regression you wish to test, normally one that assumes linearity and so we would have our normal $\beta$s telling us the slope of a predictor instead of giving as a value at each level of it. This model only works if the variables you are testing are categorical in this sense. There must be multiple observations at each level. Below we have run this test on our variables that fit these requirements. Note that sqft_living and sqft_lot are not run as factor variables because they are continuous and there are not multiple observations at each level.  

```{r, echo=FALSE}
anova(kc4.lm.full2,kc4.lm.red2)
```

  Here we find that we reject the null hypothesis, that the model fits, because our p-value is near 0, thus implying that the model does not fit. So instead of assuming a linear relationship with these stepped variables we will instead consider them as factor variables, finding the mean price at each given level, for example the mean price for a home with 5 beds and baths. The results for such a regression are shown below.  

```{r, echo=FALSE}
round(summary(lm(log(price) ~ log(sqft_living) + log(sqft_lot) + factor(as.integer(floors)) + factor(waterfront) + factor(view) + factor(condition) + factor(grade) + factor(bedbathi), data=kc_data.train))$coef, digits=4)
```

  Now we have significantly more coefficients as we have one for each factor level, or really we have one less than that as the intercept term contains all of the base factor levels, such as view=0 (not having a view) and   grade=2 (the lowest grade in the data set). Some interesting notes are that grade is only significant for it's bottom three and final two (3,4,5 and 12 and 13), which implies that grades 6 through 11 all have the same effect on home price. So only when a home's grade is really high will it be important benefit in the price. Similarly condition is only significant for its two highest levels 4 and 5, so again only if the condition of a home is extraordinary will it be associated with a benefit in the sale price. when we look a the bedbath coefficients we can see them somewhat more like cutoffs. 6 is not necessarily significant but 7 is, so within a certain value for bedbath there is not necessarily an extra benefit, though having more is generally better as at certain points it becomes significant. View we find significant at every level and has positive coefficients so each factor higher in view is associated with an increase in price. This regresion provides a fairly good fit since we are only looking at specific factor levels, but having so many coefficients is not always helpful and we may instead want to look for some functional form for these variables so we have also decdided to look at Generalized Additive Models. 


##Generalized Additive Model
  Generalized Additive Models help us get around problems of functional form. It is hard to tell what the form of each variable is specifically, but the GAM does not require us to pick a specific form (like linear) instead it uses unknown smooth functions. The GAM is of the form $g(E(Y)) = \beta_0 + f_1(x_1) + f_2(x_2) + f_3(x_3) + ... + . f_p(x_p)$ where $g$ is some link function, in our case that will  be a log function for price, $\beta_0$ is the intercept and $f_i$ is the functional form for each predictor $x_i$. These functional forms can range from parametric to non-parametric. This allowance for smoother functions lets us deal with our data that is not clearly linear, but still allows us to define some functional form to it so that we can see an overall realationship. GAMs hold the same assumption as our normal linear models, except they relax two of them. First, and most obviously, is the need to use defined functional forms instead allowing us to using potentially non-parametric smoothers. Second, GAMs also have the flexibility to permit the use non-normal error distributions In our regression below we have used splines not unlike the ones we used above.  


```{r, echo=FALSE}

kc.gam <- gam(log(price) ~ s(sqft_living, df=6) + s(sqft_lot, df=6) + as.integer(floors) + waterfront + s(view, df=6) + s(condition, df=6) + s(grade,df=6) + s(bedbath, df=6), data=kc_data.train)
summary.Gam(kc.gam)

```
  
  The summary of the model above illustrates one of the major trade offs you make using GAMs, which is a loss of interpretation. We no longer have clear coefficients that illustrate the  effect of each variable. Instead we have a series of F-tests showing that each individual predictor, in its form, is importantant to the model as they are all statistically significant at a .1% level. However, it is somewhat hard to tell what that effect is. Here we have used splines with six degrees of freedom for every varaiable, except floors and waterfront. That is because floors has only three levels 1, 2, and 3 floors, so a spline could not be created with such few levels and waterfront only has values of 0 or 1 so it does not make sense to use a spline as it simply turns "on" or "off". In order to better understand the effects of each predictor we have graphed the functions from the regression below.  


```{r, echo=FALSE} 
par(mfrow=c(1,3))
plot(kc.gam,se = TRUE, ylab="Change in log(price)")


```

  Now we can get at least some interpretation of the effect of each variable. We don't have specific coefficients, but we do have graphs. We can see that increasing the square footage of living space is always associated with a positive bump in price, but begins to taper off after awhile. The square footage of the lot on the other hand seems to start off slowly and then increase greatly in his positive association with price. Floors, is a simply linear function, for each level since we could not use a spline, but we do find that it is a negative effect. Waterfront is similarly linear, and is associated with higher prices. View is strange, it always associated with higher prices, but dips down at the second and third level, suggesting that once you have a view the price does not benefit much more until it becomes the best possible view, but still it is a somewhat strange result. Prices increase significantly as condition increases, crosing out of a negative effect at level 4. Grade has a very similar relationship with price. Finally bedbath starts off being associated with high prices, but that association, while still positive, tapers of dramatically. It is important to remember that these effects hold the other variables constant, which can help explain some of the results we have seen. If the square footage of the lot is held constant it makes sense that having a bigger and bigger home on it might have diminishing returns. While having the same sized home on a bigger and bigger lot could have greater returns. For bedbath, if we have the same sized home and keep adding more and more bedrooms and bathrooms to it, then eventually the home will be overpopulated with them so the negaive function makes a bit more sense. 



```{r, echo=FALSE}
plot(log(kc_data.test$price), predict(kc_mlr.lm, newdata=kc_data.test), type="p", xlab="Observed Log(Price)", ylab="Predicted Log(Price)")
title(main="Observed Price v. Predicted Price")
points(log(kc_data.test$price), predict(kc.gam, newdata=kc_data.test), col="red")
```
  The plot above attempts to evaluate whether this new Generalized Additive Model is indeed any better than the model we got from multiple linear regression. This plot is of the same form as the on we used to evaluate the ridge regression and lasso models. Here the original multiple linear regression's predictions are plotted in lack and the GAM predictions are in red. The predicions are again made on test data, while the model is created on training data. We are looking for a 1-1 relaionship, the predicted values are the same as the true values so we want the points to clustered around a line with the slope of 1. It appears that the red points are slightly more clusted and centered the black ones, suggesting that the GAM is better at predicting. To ensure that is the case, below we have also found the AIC for the multiple linear regression and generalized additive models. They are 14038.84 and 13137.35 respectively. Therefore, the generalized additive model did indeed do a better job of predicting. 
```{r, echo=FALSE, include=FALSE}
AIC(kc_mlr.lm)
AIC(kc.gam)
```


##Summary
  One of the most interesting findings from our analysis is how non-linear the relationships between predictors and price is. In fact practicially none of our predictors have a linear relationship with price. We have gone through a number of steps to attempt to navigate this reality, through data transformations, added-variable plots, lack of fit tests, and finally ending with the generalized additive model. That generalized additive model, that we detailed through above, is the model that we would recommend to observe the relationships between the highlighted predictors and housing prices. Specifically that model is $$log(price) = \beta_0 + f_1(sqftliving) + f_2(sqftlot) + \beta_3floors + \beta_4waterfront + f_5(view) + f_6(condition) + f_7(grade) + f_8(bedbath)$$ As we have seen from the graphs above we are able to glean some level of the relationship between each predictor and price. Some of the more interesting results being that increasing the size of a home is only so beneficial and at some point the effect of the larger home on the same lot begins to taper off. We also found that condition and grade have deterimental effect at lower levels and positive effects only at extremely high levels. We also found that an increase in the number of floors is associated with a decrease in price, suggesting that people might prefer single floored homes rather than having multiple floors, assuming the overall square footage is the same. Furthermore, packing a home with bedrooms and bathrooms does not have a consistently positive effect, suggesting that perhaps people value having larger common rooms than more sleeping quarters. 
  
  The generalized additive model is the best model for us to use because it allows us to easily deal with non-linear relationships, while still being able to glean information. Unlike the factor level model, we are still able to see some overall relationship, rather than having coefficeints for each individual level, though that had some better interpretability. Below is the residual plot for the generalized additive model. 
  
```{r, echo=FALSE}
ggplot(kc_data.train, aes(x=predict(kc.gam, newdata=kc_data.train), y=resid(kc.gam))) + geom_boxplot(aes(group=cut_width(predict(kc.gam, newdata=kc_data.train), .5))) + labs(x = "Fitted Values", y = "Std. Residuals", title = "Residual Plot")
```
  
  The Y axis has the residuals and the X axis has the fitted values. The boxplots are used to making the residuals easier to read. Each boxplot is of the same width. As you can see the residuals definitely seem to be centered around zero. Furthermore there seems to be a fairly even spread above and below 0. Lastly the variance does appear to be fairly constant throughout the plot. Since the residual plot seems fairly satisfactory and becase of the reasons stated above we decided to go with a generalized additive model.

  Finally, we believe our model can be extroplated to all home sales in Washington state, and too a somewhat lesser degree, but still siginficantly, all home sales in the United States. While the housing markets certainly differ between states, we do not believe there is an particular reason our results in Washington could not be extended to say Florida. The model would probably not predict quite as well due to potentially different clientelle or tastes in that market, but the overall predictions and relationships would likely hold true. For example, though, people who live in King's County, Washington, may be older and thus having a multi-floored home could prove difficult while somewhere with a younger population may not have quite the same relationship. That is a pure hypothetical intended to reveal potential issues in transferring the model beyond King County, however we still believe that our results speak to some trends in the housing market as a whole. 




